{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to estimate parameters of a model\n",
    "\n",
    "Lets us again consider \n",
    "1. *LINEAR REGRESSION* model.  \n",
    "    $$ h_{w,b}(x) = y = b + w_1 x_1 + w_2 x_2 + ... + w_m x_m $$ \n",
    "    compactly written as $$ h_{w,b}(x) = y = b + \\Sigma_{i=1}^m w_i x_i $$\n",
    "\n",
    "2. *LOGISTIC REGRESSION* model.\n",
    "    $$Pr(y = 1 |x) = \\frac{1}{1 + e^{-z}}$$ \n",
    "    where $$ z = b + \\Sigma_{i=1}^m w_i x_i $$\n",
    "Now that we have data and labels, our job is to come with values of paramenters i.e. $b$ and $w$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume a single variable LINEAR REGRESSION model with bias $b = 0$.\n",
    "$$ \\Rightarrow y = b + w_1 x_1 $$ or $$ y = w_1 x_1 $$\n",
    "We have a lot of data points, and for different value of $w_1$ we get different lines passing through origin.  \n",
    "**Loss function** is denoted by $J(w, b)$.  \n",
    "Loss function in case of LINEAR REGRESSION is calculated as distance of each point from the model or the line. Mathematically, for a single point, error of $i^{th}$ point = $(h_{w,b}(x^{(i)}) - y^{(i)})^2$.  \n",
    "So, $$ J(w,b) = \\frac{1}{2} \\Sigma_{i=1}^n(h_{w,b}(x^{(i)}) - y^{(i)})^2$$  \n",
    ">Here $\\frac{1}{2}$ is multiplied only for mathematical convenience.\n",
    "\n",
    "In case of Linear Regression. $$J(w,b) = \\frac{1}{2} \\Sigma_{i=1}^n([b + w_1 x_1^{(i)}] - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE**: LOGISTIC REGRESSION \n",
    "\n",
    "Considering binary classification scenario.\n",
    "\n",
    "| ACTUAL ($y$) | PREDICTED ($\\hat{y}$) | Comment |\n",
    "| --- | --- | --- |\n",
    "| 1 | 0 | Error |\n",
    "| 0 | 1 | Error |\n",
    "| 1 | 1 | OK (no error) |\n",
    "| 0 | 0 | OK (no error) |\n",
    "\n",
    "\n",
    "Loss when $y = 1$ is $-y \\log(p)$ and when $y = 0$ loss is $-(1 - y) \\log(1 - p)$ where $\\hat{y} \\Leftrightarrow p$. \n",
    "\n",
    "We get Cross-entropy loss $$ = -y \\log(p) - (1 -y) \\log(1 - p)$$\n",
    "\n",
    ">For Multi-class classification poblem we use *Categorical Cross-Entropy Loss* and *Sparse Categorical Cross-Entropy Loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the value of parameter, we need to minimize loss and find **optimal** values of parameters. This is where **optimization techniques** are used. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
